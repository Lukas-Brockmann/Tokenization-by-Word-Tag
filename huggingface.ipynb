{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sage_tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msage_tokenizer\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sage_tokenizer'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from IPython import get_ipython\n",
    "\n",
    "sys.path.append(os.path.abspath(\"./lib\"))\n",
    "if 'autoreload' not in get_ipython().magics_manager.magics['line']:\n",
    "    %load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from lib import dataloading as dl\n",
    "from lib import tokenizer as tk\n",
    "import tokenizers\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sage_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 2810 rows with non-UPOS tags \n",
      "Tags dropped: ['_']\n",
      "            FORM         LEMMA   UPOS XPOS        FEATS HEAD DEPREL  \\\n",
      "ID                                                                    \n",
      "1      Aesthetic     aesthetic    ADJ   JJ   Degree=Pos    2   amod   \n",
      "2   Appreciation  appreciation   NOUN   NN  Number=Sing    0   root   \n",
      "3            and           and  CCONJ   CC            _    5     cc   \n",
      "4        Spanish       Spanish    ADJ   JJ   Degree=Pos    5   amod   \n",
      "5            Art           art   NOUN   NN  Number=Sing    2   conj   \n",
      "\n",
      "          DEPS                                               MISC  \n",
      "ID                                                                 \n",
      "1       2:amod  Discourse=organization-heading:1->57:8:grf-ly-...  \n",
      "2       0:root                       Entity=1)|MSeg=Appreciat-ion  \n",
      "3         5:cc                                                  _  \n",
      "4       5:amod     Entity=(2-abstract-new-cf2-2-sgl|MSeg=Span-ish  \n",
      "5   2:conj:and                            Entity=2)|SpaceAfter=No  \n"
     ]
    }
   ],
   "source": [
    "data_df = dl.load_conllu(\n",
    "    r\"D:\\Dropbox\\Bachlorarbeit\\Datasets\\Universal Dependencies 2.15\\ud-treebanks-v2.15\\UD_English-GUM\\en_gum-ud-train.conllu\"\n",
    ")\n",
    "data_df = dl.clear_non_UPOS_tags(data_df)\n",
    "print(data_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "upos_tags = [\n",
    "        \"ADJ\",\n",
    "        \"ADP\",\n",
    "        \"ADV\",\n",
    "        \"AUX\",\n",
    "        \"CCONJ\",\n",
    "        \"DET\",\n",
    "        \"INTJ\",\n",
    "        \"NOUN\",\n",
    "        \"NUM\",\n",
    "        \"PART\",\n",
    "        \"PRON\",\n",
    "        \"PROPN\",\n",
    "        \"PUNCT\",\n",
    "        \"SCONJ\",\n",
    "        \"SYM\",\n",
    "        \"VERB\",\n",
    "        \"X\",\n",
    "    ]\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UPOS\n",
       "ADJ      0.065591\n",
       "ADP      0.094304\n",
       "ADV      0.047767\n",
       "AUX      0.053751\n",
       "CCONJ    0.032844\n",
       "DET      0.081215\n",
       "INTJ     0.009768\n",
       "NOUN     0.166281\n",
       "NUM      0.019317\n",
       "PART     0.023972\n",
       "PRON     0.084079\n",
       "PROPN    0.058059\n",
       "PUNCT    0.138616\n",
       "SCONJ    0.016136\n",
       "SYM      0.001627\n",
       "VERB     0.104742\n",
       "X        0.001932\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[\"UPOS\"].value_counts(normalize=True).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for algorithm in [\"BPE\", \"WordPiece\", \"Unigram\"]:\n",
    "    path = Path(r\"D:\\Dropbox\\Bachlorarbeit\\Tokenization by Word Tag\\Tokenization by Word Tag Tokenizers\") / f\"upos_{algorithm.lower()}_tokenizer.json\"\n",
    "    tokenizer = tk.train_and_merge_tokenizers(\n",
    "        data_df,\n",
    "        tokenizer_algorithm=algorithm,\n",
    "        vocab_size=1000,\n",
    "        allocation=\"proportional\",\n",
    "        save_path=str(path)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "tokenizer = tk.train_tokenizer(data_df[\"FORM\"].values.tolist(), vocab_size, algorithm=\"bpe\")\n",
    "tokenizer.save(r\"D:\\Dropbox\\Bachlorarbeit\\Tokenization by Word Tag\\Tokenization by Word Tag Tokenizers\\classic_bpe_tokenizer.json\")\n",
    "\n",
    "tokenizer = tk.train_tokenizer(data_df[\"FORM\"].values.tolist(), vocab_size, algorithm=\"wordpiece\")\n",
    "tokenizer.save(r\"D:\\Dropbox\\Bachlorarbeit\\Tokenization by Word Tag\\Tokenization by Word Tag Tokenizers\\classic_wordpiece_tokenizer.json\")\n",
    "\n",
    "tokenizer = tk.train_tokenizer(data_df[\"FORM\"].values.tolist(), vocab_size, algorithm=\"unigram\")\n",
    "tokenizer.save(r\"D:\\Dropbox\\Bachlorarbeit\\Tokenization by Word Tag\\Tokenization by Word Tag Tokenizers\\classic_unigram_tokenizer.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
